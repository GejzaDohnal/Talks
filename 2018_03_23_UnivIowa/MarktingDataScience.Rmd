---
title: "Marketing Data Science"
author: "[Hui Lin](http://scientistcafe.com), DowDuPont"
date: "2018/03/23 @ University of Iowa"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# About Me

- Hui Lin, Data Scientist (http://scientistcafe.com)
- Contact：
    - Email：longqiman@gmail.com
    - Twitter：[@gossip_rabbit](https://twitter.com/gossip_rabbit)
    - LinkedIn：https://www.linkedin.com/in/hui-lin-81653855/

---

# Outline

- Slides: http://scientistcafe.com/IDS/slides/IntroToDataScience.html
- What is marketing data science? What value can data science bring?
- Predictive Analytics: parametric model and ensemble method
- Unstructured data analytics: digital marketing
- Market research analytics
- Case Study: Group Lasso Logistic Regression for Customer Retention
- Data Scientist Skill Set & How can you get in?

---

class: inverse, center, middle

# What is data science? 

---

## 

<img src="images/DataScienceTrend2004.PNG" alt="HTML5 Icon" style="width:900px;height:500px;">

---

## What is data science? 

<img src="images/awesome-me.gif" alt="HTML5 Icon" style="width:600px;height:400px;">

---

## Brief History

<center>
![](http://scientistcafe.com/book/Figure/DataScienceTimeline.png){width=90%}
</center>

---

## What questions can data science answer? 

- Specific
    1. How can we increase sales?
    1. Does the January campaign on product X increase the amount of purchase from our 2017 retained customers? 

- Data
    1. Representative
    1. Relevant
    1. Quality
    
---

## Types of Questions

<center>
![](http://scientistcafe.com/book/Figure/DataScienceQuestion.png){height=55%}
</center>

---

## Types of Learning

<center>
![](http://scientistcafe.com/book/Figure/LearningStyles.png){height=100%}
</center>

---

## Types of Algorithm (1) 

- http://scientistcafe.com/2017/07/08/MachineLearningAl.html

<center>
![](images/AlogrithmTypes1.png){height=100%}
</center>

---

## Types of Algorithm (2)

- http://scientistcafe.com/2017/07/08/MachineLearningAl.html

<center>
![](images/AlogrithmTypes2.png){height=100%}
</center>

---

## Types of Algorithm (3)

- http://scientistcafe.com/2017/07/08/MachineLearningAl.html

<center>
![](images/AlogrithmTypes3.png){height=100%}
</center>

---

class: inverse, center, middle

# Predictive Analytics: parametric model and ensemble method

---

## Predictability v.s. Interpretability

---

## Systematic Error and Random Error

---

class: inverse, center, middle

# Program and Service Analysis

---

## Causal inference in observational environment

---
class: inverse, center, middle

# Unstructured data analytics: digital marketing

---

## Where to get the data?

- API: Twitter/Google/Wikipedia...
- Webpage: Forum, Reviews
- Survey
- Interviews

---

## How does computer understand the language?

---

## Example of a Marketing Campaign
    
---

class: inverse, center, middle

# Market research analytics

---

## Choice-based conjoint analysis

---

## Customer perception analysis 
    
---

class: inverse, center, middle

# Case Study: Group Lasso Logistic Regression for Customer Retention

---

## Case Study

- General context without going into business details

- Focus on the technical parts

- Follow the data science project cycle

---

## Project Cycle

![](images/GeneralProcessEN.png)

---

## Business Questions

1. How likely will a customer purchase?
2. What are the key drivers?

---

## Clarification Questions

- Who are our customers?
- Are there different segments of customers?
- What is a purchase?  
- How far ahead do we need to predict?
- What are the predictors? 
- What is the quality of the data?
- Where are different data sets located?
- ...

---

## Refined Question

- Response: if a corn multi-year customer will purchase again next year

- Predictors: other customer experience and behavior data

---

## Project Cycle

![](images/GeneralProcessEN.png) 

---

## Data Preprocessing

- Cleaning
- Missing values
- Transformation
    - Categorical
    - 0/1
    - percentage 
    - large positive number 
    - counts
    - $x_{ij}^{*} = \frac{x_{ij} - quantile(x_{.j}, 0.01)}{quantile(x_{.j, 0.99})- quantile(x_{.j}, 0.01)}$

---

## Project Cycle

![](images/GeneralProcessEN.png) 

---

## Multivariate logistic regression

- $\mathbf{y}=(y_{1},y_{2},\dots,y_{n})^{T}$  binary response vector
- $X=(\mathbf{x_{1}},\mathbf{x_{2}},\dots,\mathbf{x_{n}})^{T}$ design matrix in which each $\mathbf{x_{i}}$ is $p+1$ dimention column 
- $\mathbf{\beta}=(\beta_{0},\dots,\beta_{p})^{T}$ parameter vector

- The logliklihood function is as follows:

$$ln\mathcal{L}(\boldsymbol{\beta}|\mathbf{y})=\sum_{i=1}^{n}\left\{ y_{i}ln\frac{1}{1+exp(-\mathbf{x_{i}}^{T}\mathbf{\boldsymbol{\beta}})}+(1-y_{i})ln\left[1-\frac{1}{1+exp(-\mathbf{x_{i}}^{T}\boldsymbol{\beta})}\right]\right\} $$

$$D(\boldsymbol{\beta})\equiv\frac{\partial ln\mathcal{L}(\boldsymbol{\beta}|\mathbf{y})}{\partial\boldsymbol{\beta}}=\sum_{i=1}^{n}\left\{ y_{i}-\frac{1}{exp(-\mathbf{x_{i}}^{T}\boldsymbol{\beta})}\right\} \mathbf{x_{i}}$$

- Problems: quasi-complete-separation and significance based variable selection
- Solution: add penalty

---

## Lasso: weighted L1-norm penalty [Tibshirani 1996]

- $\hat{\beta}_{\lambda}=argmin_{\beta}(\parallel\mathbf{Y-X}\beta\parallel_{2}^{2}+\lambda\Sigma_{j=1}^{p}|\beta_{j}|)$
- Advantage: stabilize the estimation, also a variable selection tool
- Limitation: only selects individual dummy variables, the estimates are affected by the way dummy variables are encoded (M. Yuan and Y. Lin, Model selection and estimation in regression with grouped variables, J. R. Stat. Soc. Ser. B Stat. Methodol. 68 (2007), pp.49-67)

---

## Group Lasso Logistic Regression

- $\mathbf{x_{i,g}}$ vector of dummy variables ( $i^{th}$ observation in group $g$ ) $i = 1,...,n , g = 1,...,G$

- $y_{i}$ binary response for the $i^{th}$ observation

- $df_{g}$ degrees of freedom of group $g$

$$\mathcal{S}_{\lambda}(\beta)=-l(\beta)+\lambda\sum_{g=1}^{G}s(df_{g})\parallel\beta_{g}\parallel_{2}$$

where $l(\mathbf{\beta})$ is log-likelihood:

$$\Sigma_{i=1}^{n}\{y_{i}\eta_{\beta}(\mathbf{x_{i}})-log[1+exp(\eta_{\beta}(\mathbf{x_{i}}))]\}$$


$\lambda$ tuning parameter for penalty and $s(\centerdot)$ is $s(df_{g})=df_{g}^{0.5}$

---

## Performance Measure

- Maximize AUC
- Grid of 148 values $${0.96\lambda_{max},0.96^{2}\lambda_{max},\dots,0.96^{148}\lambda_{max}}$$ 

where 

$$\lambda_{max}=max_{g\in {1,\dots,G}}{\frac{1}{s(df_{g})}\parallel \mathbf{x_{g}^{T}(y-\bar{y})}\parallel_{2}}$$

---

## Model Training and Testing


- 70/30 (Train/Test)
- 10-fold cross validation (Train)
- Double check: one year holdout

---

## Cut-off Tuning
 
1. Ordered the score from high to low
2. Calculate the sensitivity and specificity as the cutoff changes
3. Get the cut-off values with corresponding likelihoods

---

## Project Cycle

![](images/GeneralProcessEN.png) 

---

## Model Comparison

- Traditional Stepwise Regression
- Random Forest
- SVM
- Neural Network

> Essentially, all models are wrong, but some are useful.

---

## Project Cycle

![](images/GeneralProcessEN.png)

---
class: inverse, center, middle

# Be a data scientist!

---

## Data Scientist Skill Set

![](http://scientistcafe.com/book/Figure/SkillEN.png){width=100%}

---

## Some links

- [Types of Machine Learning Algorithm](http://scientistcafe.com/2017/07/08/MachineLearningAl.html)

- Online books:
    - [The Elements of Statistical Learning](http://web.stanford.edu/~hastie/ElemStatLearn/)
    - [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
    - [Introduction to Data Science](http://scientistcafe.com/IDS/)(still writing)

- Hard copy books:
    - [Applied Predictive Modeling](http://appliedpredictivemodeling.com)
    - [R for Marketing Research and Analytics](http://r-marketing.r-forge.r-project.org)
    - [套路！机器学习](http://scientistcafe.com/book/)

- [Awesome-Data-Science-Materials](https://github.com/happyrabbit/Awesome-Data-Science-Materials)